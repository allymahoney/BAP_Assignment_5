{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up\n",
    "Some clarification on assignment\n",
    "[CodePen html practice](https://codepen.io/careybaldwin02/pen/wOxvga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "[requests library documentation](http://docs.python-requests.org/en/master/)  \n",
    "[BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)  \n",
    "[CodePen](https://codepen.io/careybaldwin02/pen/wOxvga)  \n",
    "[epicurious](http://www.epicurious.com/)  \n",
    "[Market Watch](https://www.marketwatch.com/investing/stock/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Become familiar with HTML and CSS through hands on example\n",
    "- Review and practice HTTP requests\n",
    "- Understand the purpose of web-scraping and review some legal, ethical considerations\n",
    "- Learn to import and use the BeautifulSoup library to \n",
    "- build objects from HTML pulled from webpages using bs\n",
    "- use the bs built in functions for getting data by tag and class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "When possible, it is much more efficient to extract data from an API (JSON data).  However, this may not always be an option.  Web scraping is gathering data from a webpage, often using a program.  We will be using the BeautifulSoup library along with the skills we have developed to write functions that help us target and format the data we might want from a webpage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caveat\n",
    "It is important to note that webpages change all the time and a program that we write today, may note work in the same way tomorrow.  \n",
    "\n",
    "#### Legal and ethical issues\n",
    "- often against the 'Terms of Use' of a web site\n",
    "- factual. non-proprietary data is usually ok\n",
    "- proprietary data scraping depends on your objective\n",
    "- potential damages to the site\n",
    "- public vs. private information\n",
    "- be open about the information extraction\n",
    "- is there a public interest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools for web scraping\n",
    "- requests: handles the HTTP requests and responses\n",
    "- Beautiful Soup: utilizes the 'tag structure' of an html page to quickly parse the contents of a page and retrieve data\n",
    "- Selenium: pretends to be a browser.  Useful for pages with scripts (extra functionality, interactive components)\n",
    "\n",
    "Python cannot understand JavaScript.  Selenium can pretend to be a browser so the server doesnâ€™t know that the request is coming from a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use Beautiful Soup\n",
    "- HTML (and XML) parser - a parser builds a data structure (tree) from a language\n",
    "- Uses 'tags'\n",
    "- Creates a parse tree\n",
    "- Can handle incomplete tagging\n",
    "- Tags organized in hierarchical dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The http request response cycle\n",
    "We can send a request to a url while specifying keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.epicurious.com/search/Apple Pie\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- results_page is the object from which we need to extract the data.    - 'lxml' is the library we use for parsing the information.  \n",
    "- We don't need the %20 because the requests library adds that in for us.  \n",
    "- The prettify() function just makes the output look nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the same program but with a dynamic input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = input(\"Please enter the things you want to see in a recipe \")\n",
    "url = \"http://www.epicurious.com/search/\" + keywords\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Failure\")\n",
    "    \n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the BeautifulSoup object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content is all of the HTML for the page.  Let's look for our keywords in the HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_page=BeautifulSoup(response.content,'lxml')\n",
    "print(results_page.prettify)\n",
    "\n",
    "# Ctrl+f to search this page for our keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Requests\n",
    "Write a program, like the one above, to the market watch website:  https://www.marketwatch.com/investing/stock/.  Make your program dynamic so that it can accept an input that is a stock symbol as a keyword, e.g. aapl, and add it to the url so that the url becomes:  https://www.marketwatch.com/investing/stock/aapl, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the response using the BeautifulSoup module and print the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BS4 Functions\n",
    "These are some useful functions we can use to navigate the tree and extract information. \n",
    "\n",
    "|Function|Usage|\n",
    "|-----|-----|\n",
    "|```<tag>.find(<tag_name>, attribute=value)```|finds the first matching child tag|\n",
    "|```<tag>.find_all(<tag_name>, attribute=value)```|finds all matching child tags|\n",
    "|```<tag>.get_text()```|returns the marked up text|\n",
    "|```<tag>.parent```|returns the (immediate) parent|\n",
    "|```<tag>.parents```|returns all parents|\n",
    "|```<tag>.children```|returns the direct children|\n",
    "|```<tag>.descendents```|returns all children|\n",
    "|```<tag>.get(attribute)```|returns the value of the specified attribute|\n",
    "\n",
    "note: an attribute can be something like the value of an href inside a tag:\n",
    "```\n",
    "<a href=www.google.com>Google</a>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't use all of these functions, but some will be very useful.  The find_all() function, for example finds all instances of a specified tag.  It returns what's called a result_set, which we can see is a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a_tags = results_page.find_all('a')\n",
    "print(type(all_a_tags))\n",
    "print(all_a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find() will find the first instance of a specified tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_tag = results_page.find('div')\n",
    "print(div_tag)\n",
    "print(type(div_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of this bs4 element tag, it is a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs4 functions can be applied recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_tag.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can't do the following:\n",
    "```\n",
    "all_a_tags.find('div')\n",
    "```\n",
    "because the all_a_tags object is a result_set, not a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_a_tags.find('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:  Scraping HTML Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_page=\n",
    "\n",
    "# print(stocks_page.prettify)\n",
    "\n",
    "# Find all a tags in the stocks_page results\n",
    "stock_a= \n",
    "\n",
    "# Find the first div tag in the stocks_page results\n",
    "stock_div= \n",
    "\n",
    "# Find the a tags within div tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualifying Data Extraction by Class\n",
    "I can add a level of specificity to the selection of a tag by adding the value of a particular class.  Both find as well as find_all can be qualified by css selectors.\n",
    "- using selector=value\n",
    "- using a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine a webpage, we can find the tag and class name that uniquely identifies the data we want.  \n",
    "\n",
    "We might want to specify the article tag by class = \"recipe-content-card\"\n",
    "\n",
    "The underscore is there because \"class\" is a keyword in python.  BeautifulSoup understands the class_ syntax  \n",
    "\n",
    "We can take a look at what comes back and check the length to see how many items on the page fit the criteria we specified in our request.\n",
    "\n",
    "To summarize, what we are doing here is figuring out what uniquely identifies the data that we are looking for. This is often found in the css selector values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#When using this method and looking for 'class' use 'class_' (because class is a reserved word in python)\n",
    "#Note that we get a list back because find_all returns a list\n",
    "print(results_page.find_all('article',class_=\"recipe-content-card\"))\n",
    "# I can check the length of this to see how many of these I get back\n",
    "len(results_page.find_all('article',class_=\"recipe-content-card\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to the method above where we call the find_all() function with the two arguments, we can also send the arguments as dictionaries.  This is useful when we want to look for multiple selectors.  Then we can include the selectors as values of the class key in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we're using a string as the key, the fact that class is a reserved word is not a problem\n",
    "#We get an element back because find returns an element\n",
    "results_page.find('article',{'class':'recipe-content-card'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting text\n",
    "\n",
    "Given a particular tag, we can get the content, for example, I find the first article with the class \"recipe-content-card\".  .get_text() gives us the text in that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_page.find('article',{'class':'recipe-content-card'}).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the actual value of an attribute.  A common task is extracting all links from a page.  get() returns the value of a tag attribute returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 'article' tag with the class 'recipe-content-card' and name it recipe_tag\n",
    "recipe_tag = results_page.find('article',{'class':'recipe-content-card'})\n",
    "# create an object called recipe_link that retrieves the a-tag from within the recipe_tag object\n",
    "recipe_link = recipe_tag.find('a')\n",
    "print(\"a tag:\",recipe_link)\n",
    "# from within the recipe_link object, retrieve the value of the href attribute\n",
    "link_url = recipe_link.get('href')\n",
    "print(\"link url:\",link_url)\n",
    "print(type(link_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that the above url would need an http in order to become an active link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:  Extracting Content\n",
    "Find the part of the Market Watch webpage that contains the current stock value in large font.  Reference the website:  https://www.marketwatch.com/investing/stock/aapl  Then return the associated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the appropriate tag name and class name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text associated with the current stock value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:  Extracting Attribute Values\n",
    "At the top left side of the page:  https://www.marketwatch.com/investing/stock/aapl, there is a table containing realtime stock values.  Inspect the part of the page containing the table of stock values.  See if you can find an attribute called \"channel\".  Let's dig into this a bit more and see what it means.  Let's see if we can extract the \"channel\" values in the way we extracted the \"href\" attribute values for recipes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference the stocks_page object we defined in the exercise above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
